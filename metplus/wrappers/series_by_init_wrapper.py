'''! @namespace SeriesByInitWrapper
@brief Performs any optional filtering of input tcst data then performs
regridding via the MET tool regrid_data_plane, then builds up
the commands to perform a series analysis by init time by invoking the
MET tool series_analysis. NetCDF plots are generated by invoking the MET tool
plot_data_plane. The NetCDF plots are then converted to .png and Postscript.

Call as follows:
@code{.sh}
series_by_init.py [-c /path/to/user.template.conf]
@endcode
'''

import errno
import os
import re
import sys
from datetime import datetime

from ..util import met_util as util
from ..util import ti_calculate, do_string_sub
from .plot_data_plane_wrapper import PlotDataPlaneWrapper
from . import CommandBuilder

class SeriesByInitWrapper(CommandBuilder):
    """!  Performs series analysis based on init time by first performing any
          additional filtering via the wrapper to the MET tool tc_stat,
          tc_stat_wrapper.  Next, the arguments to run the MET tool
          series_analysis is done
    """
    # class variables to define prefixes for intermediate files
    FCST_ASCII_FILE_PREFIX = 'FCST_ASCII_FILES_'
    ANLY_ASCII_FILE_PREFIX = 'ANLY_ASCII_FILES_'

    def __init__(self, config, instance=None, config_overrides={}):
        self.app_path = os.path.join(config.getdir('MET_BIN_DIR', ''),
                                     'series_analysis')
        self.app_name = os.path.basename(self.app_path)
        super().__init__(config,
                         instance=instance,
                         config_overrides=config_overrides)

        self.plot_data_plane = self.plot_data_plane_init()

        self.logger.debug("Initialized SeriesByInitWrapper")

    def create_c_dict(self):
        c_dict = super().create_c_dict()
        c_dict['MODEL'] = self.config.getstr('config',
                                             'MODEL',
                                             'FCST')
        c_dict['REGRID_TO_GRID'] = (
            self.config.getstr('config',
                               'SERIES_ANALYSIS_REGRID_TO_GRID',
                               '')
        )

        # get stat list to loop over
        c_dict['STAT_LIST'] = util.getlist(
            self.config.getstr('config',
                               'SERIES_ANALYSIS_STAT_LIST',
                               '')
        )
        if not c_dict['STAT_LIST']:
            self.log_error("Must set SERIES_ANALYSIS_STAT_LIST to run.")

        # set stat list to set output_stats.cnt in MET config file
        self.set_c_dict_list(c_dict,
                             'SERIES_ANALYSIS_STAT_LIST',
                             'cnt',
                             'OUTPUT_STATS_CNT')

        c_dict['TILE_INPUT_DIR'] = self.config.getdir('SERIES_ANALYSIS_TILE_INPUT_DIR',
                                                 '')
        if not c_dict['TILE_INPUT_DIR']:
            self.log_error("Must set SERIES_ANALYSIS_TILE_INPUT_DIR")

        c_dict['STAT_INPUT_DIR'] = (
            self.config.getdir('SERIES_ANALYSIS_STAT_INPUT_DIR', '')
        )

        c_dict['STAT_INPUT_TEMPLATE'] = (
            self.config.getraw('config',
                               'SERIES_ANALYSIS_STAT_INPUT_TEMPLATE')
        )
        if not c_dict['STAT_INPUT_TEMPLATE']:
            self.log_error("Must set SERIES_ANALYSIS_STAT_INPUT_TEMPLATE")

        c_dict['OUTPUT_DIR'] = self.config.getdir('SERIES_ANALYSIS_OUTPUT_DIR',
                                                  '')
        c_dict['OUTPUT_TEMPLATE'] = (
            self.config.getraw('config',
                               'SERIES_ANALYSIS_OUTPUT_TEMPLATE')
        )
        if not c_dict['OUTPUT_DIR']:
            self.log_error("Must set SERIES_ANALYSIS_OUTPUT_DIR to run.")

        c_dict['FILTERED_OUTPUT_DIR'] = (
            self.config.getdir('SERIES_ANALYSIS_FILTERED_OUTPUT_DIR',
                               '')
        )

        c_dict['TC_STAT_OUTPUT_TEMPLATE'] = self.config.getraw('config',
                                                               'TC_STAT_OUTPUT_TEMPLATE')

        c_dict['FCST_TILE_PREFIX'] = self.config.getstr('config',
                                              'FCST_EXTRACT_TILES_PREFIX',
                                              '')
        if not c_dict['FCST_TILE_PREFIX']:
            self.log_error("Must set FCST_EXTRACT_TILES_PREFIX")

        c_dict['ANLY_TILE_PREFIX'] = self.config.getstr('config',
                                              'OBS_EXTRACT_TILES_PREFIX',
                                              '')
        if not c_dict['ANLY_TILE_PREFIX']:
            self.log_error("Must set OBS_EXTRACT_TILES_PREFIX")

        c_dict['FCST_TILE_REGEX'] = (
            f".*{c_dict['FCST_TILE_PREFIX']}.*nc"
        )

        c_dict['ANLY_TILE_REGEX'] = (
            f".*{c_dict['ANLY_TILE_PREFIX']}.*nc"
        )

        c_dict['CONFIG_FILE'] = self.config.getstr('config',
                                                   'SERIES_ANALYSIS_CONFIG_FILE',
                                                   '')
        if not c_dict['CONFIG_FILE']:
            self.log_error("SERIES_ANALYSIS_CONFIG_FILE must be set")

        c_dict['BACKGROUND_MAP'] = self.config.getbool('config',
                                             'SERIES_ANALYSIS_BACKGROUND_MAP',
                                             False)

        c_dict['VAR_LIST'] = util.parse_var_list(self.config)
        if not c_dict['VAR_LIST']:
            self.log_error("No fields specified. Please set "
                           "[FCST/OBS]_VAR<n>_[NAME/LEVELS]")

        c_dict['GENERATE_PLOTS'] = (
            self.config.getbool('config',
                                'SERIES_ANALYSIS_GENERATE_PLOTS',
                                True)
        )

        return c_dict

    def plot_data_plane_init(self):
        # set values to allow successful initialization of PlotDataPlaneWrapper
        plot_overrides = {'PLOT_DATA_PLANE_INPUT_TEMPLATE': 'template',
                          'PLOT_DATA_PLANE_OUTPUT_TEMPLATE': 'template',
                          'PLOT_DATA_PLANE_FIELD_NAME': 'field_name',
                          'PLOT_DATA_PLANE_CONVERT_TO_IMAGE': True,
                          }

        if not self.c_dict['BACKGROUND_MAP']:
            plot_overrides['PLOT_DATA_PLANE_FIELD_EXTRA'] = (
                "map_data={ source=[];}"
            )

        pdp_wrapper = PlotDataPlaneWrapper(self.config,
                                           config_overrides=plot_overrides)
        return pdp_wrapper

    def run_at_time(self, input_dict):
        """! Invoke the series analysis script based on the init time

            @param input_dict input time dictionary
            @returns True on success, False otherwise
        """
        self.logger.debug("Starting series analysis by init time")

        # Calculate other time information from available time info
        time_info = ti_calculate(input_dict)

        storm_list = self.get_storms_for_init(time_info)
        if not storm_list:
            # No storms for this init time, check next init time in list
            self.logger.debug(f"No storms found for current init time")
            return False

        # Create FCST and ANLY ASCII files based on init time and storm id
        self.create_ascii_storm_files_list(time_info, storm_list)

        # Build up the arguments to and then run the MET tool series_analysis.
        if not self.build_and_run_series_request(time_info, storm_list):
            return False

        if self.c_dict['GENERATE_PLOTS']:
            self.generate_plots(time_info, storm_list)
        else:
            self.logger.debug("Skip plotting output. Change "
                              "SERIES_ANALYSIS_GENERATE_PLOTS to True to run "
                              "this step.")

        self.logger.debug("Finished series analysis by init time")
        return True

    def get_storms_for_init(self, time_info):
        """! Find the .tcst filter file for the current run time and get the
             list of storm IDs that are found in the file.

            @param time_info dictionary containing time information
            @returns A list of all the storms ids that correspond to the
             current init time or None if filter file does not exist
        """
        # Retrieve filter files, first create the filename
        # by piecing together the out_dir_base with the cur_init.
        filter_template = os.path.join(self.c_dict['STAT_INPUT_DIR'],
                                       self.c_dict['STAT_INPUT_TEMPLATE'])
        filter_file = do_string_sub(filter_template, **time_info)
        self.logger.debug(f"Getting storms from filter file: {filter_file}")
        if not os.path.exists(filter_file):
            self.log_error(f"Filter file does not exist: {filter_file}")
            return None

        # Now that we have the filter filename for the init time, let's
        # extract all the storm ids in this filter file.
        storm_list = util.get_storm_ids(filter_file, self.logger)

        return storm_list

    def create_ascii_storm_files_list(self, time_info, storm_list):
        """! Creates the list of ASCII files that contain the storm id and init
             times.  The list is used to create an ASCII file which will be
             used as the option to the -obs or -fcst flag to the MET
             series_analysis tool.

             @param time_info dictionary containing time information
             @param storm_list list of storm IDs to process
        """
        # get all analysis and forecast files in the tile directory
        anly_grid_files = util.get_files(self.c_dict['TILE_INPUT_DIR'],
                                         self.c_dict['ANLY_TILE_REGEX'])
        fcst_grid_files = util.get_files(self.c_dict['TILE_INPUT_DIR'],
                                         self.c_dict['FCST_TILE_REGEX'])

        if not anly_grid_files:
            self.logger.error("No gridded analysis files found. ExtractTiles "
                              "wrapper must be run first.")
            return False

        if not fcst_grid_files:
            self.logger.error("No gridded forecast files found. ExtractTiles "
                              "wrapper must be run first.")
            return False

        output_dir_template = os.path.join(self.c_dict['OUTPUT_DIR'],
                                           self.c_dict['OUTPUT_TEMPLATE'])
        output_dir_template = os.path.dirname(output_dir_template)

        for storm_id in storm_list:
            # get output directory including storm ID
            output_dir = do_string_sub(output_dir_template,
                                       storm_id=storm_id,
                                       **time_info)

            # create forecast file list
            fcst_ascii_filename = f"{self.FCST_ASCII_FILE_PREFIX}{storm_id}"
            self.create_file_list(fcst_ascii_filename,
                                  fcst_grid_files,
                                  output_dir,
                                  storm_id=storm_id)

            # create analysis file list
            anly_ascii_filename = f"{self.ANLY_ASCII_FILE_PREFIX}{storm_id}"
            self.create_file_list(anly_ascii_filename,
                                  anly_grid_files,
                                  output_dir,
                                  storm_id=storm_id)

        self.logger.debug("Finished creating FCST and ANLY ASCII files")
        return True

    def create_file_list(self, ascii_filename, grid_files, output_dir,
                         **kwargs):
        """! Filter tile file list and write list file
        @param ascii_filename output filename to write list
        @param grid_files list of file paths to filter
        @param output_dir directory to write output file
        @param kwargs filtering options. Valid variable names include -
          storm_id: ID of storm to filter
        """
        # filter tile files by filter criteria provided
        filtered_files = self.filter_tiles(grid_files,
                                           **kwargs)
        if not filtered_files:
            self.logger.warning("Filtering produces no results. No files "
                                f"will be included in {ascii_filename}")

        # write ascii file with list of filtered files
        self.write_list_file(ascii_filename,
                             filtered_files,
                             output_dir=output_dir)

    def filter_tiles(self, grid_files, **kwargs):
        """! Filter list of gridded files based on criteria provided
        @param grid_files list of files to filter
        @param kwargs filtering options. Valid variable names include -
          storm_id: ID of storm to filter
        @returns subset list of unique files
        """
        # get filtering requirements from kwargs
        if 'storm_id' in kwargs and kwargs['storm_id'] is not None:
            storm_id = kwargs['storm_id']
            self.logger.debug(f"Filtering by storm_id: {storm_id}")
        else:
            storm_id = None

        # save output files in a set to avoid duplicates
        output_set = set()
        for grid_file in grid_files:
            # filter by storm ID if requested
            if storm_id:
                # if ID not found in file path, skip this file
                if storm_id not in grid_file:
                    continue

            output_set.add(grid_file)

        # sorted converts the set to a list that is in order
        return sorted(output_set)

    def build_and_run_series_request(self, time_info, storm_list):
        """! Build up the -obs, -fcst, -out necessary for running the
             series_analysis MET tool, then invoke series_analysis.

             @param time_info dictionary containing time information for
             current run
             @param storm_list list of storms IDs to process
             @returns True if all runs succeeded, False if there was a problem
             with any of the runs
        """
        # Now assemble the -fcst, -obs, and -out arguments and invoke the
        # MET Tool: series_analysis.
        success = True
        output_dir_template = os.path.join(self.c_dict['OUTPUT_DIR'],
                                           self.c_dict['OUTPUT_TEMPLATE'])
        output_dir_template = os.path.dirname(output_dir_template)
        for storm_id in storm_list:
            # add the current storm ID to the time dictionary for string sub
            time_info['storm_id'] = storm_id

            output_dir = do_string_sub(output_dir_template,
                                       **time_info)
            fcst_path = os.path.join(output_dir,
                                     f"{self.FCST_ASCII_FILE_PREFIX}{storm_id}")
            obs_path = os.path.join(output_dir,
                                    f"{self.ANLY_ASCII_FILE_PREFIX}{storm_id}")

            # Build the -obs and -fcst portions of the series_analysis
            # command. Then generate the -out portion, get the NAME and
            # corresponding LEVEL for each variable.
            for var_info in self.c_dict['VAR_LIST']:
                self.infiles.append(f"-fcst {fcst_path}")
                self.infiles.append(f"-obs {obs_path}")
                self.add_field_info_to_time_info(time_info, var_info)
                self.set_environment_variables(time_info, var_info)

                self.find_and_check_output_file(time_info)
                self.logger.debug(
                    f'output dir for series_analysis: {self.get_output_path()}'
                )

                if not self.build():
                    success = False
                self.clear()

        return success

    def set_environment_variables(self, time_info, var_info):
        """! Set the env variables based on settings in the METplus config
             files.

             @param time_info dictionary containing time information
             @param var_info dictionary containing field information
        """
        self.logger.info('Setting env variables from config file...')

        # Set all the environment variables that are needed by the
        # MET config file.
        # Set up the environment variable to be used in the Series Analysis
        tmp_stat_string = str(self.c_dict['STAT_LIST'])
        tmp_stat_string = tmp_stat_string.replace("\'", "\"")
        os.environ['STAT_LIST'] = tmp_stat_string
        self.add_env_var('STAT_LIST', tmp_stat_string)
        #        self.add_env_var('STAT_LIST', self.c_dict.get('OUTPUT_STATS_CNT', ''))

        # set MODEL and REGRID_TO_GRID environment variables
        self.add_common_envs()

        # Set the NAME and LEVEL environment variables
        self.add_env_var('NAME', var_info['fcst_name'])
        self.add_env_var('LEVEL', var_info['fcst_level'])

        super().set_environment_variables(time_info)

    def get_command(self):
        cmd = self.app_path + " "

        cmd += ' '.join(self.infiles)

        cmd += f" -config {self.c_dict['CONFIG_FILE']}"

        if not self.get_output_path():
            self.logger.info("No output directory specified, because series "
                             "analysis has multiple directories")
            self.logger.info("No output filename specified, because "
                             "series analysis has multiple files")
        else:
            cmd += " -out " + os.path.join(self.get_output_path())

        return cmd

    def generate_plots(self, time_info, storm_list):
        """! Generate the plots from the series_analysis output.

             @param time_info dictionary containing time information
             @param storm_list list of storm IDs to process
        """
        output_dir_template = os.path.join(self.c_dict['OUTPUT_DIR'],
                                           self.c_dict['OUTPUT_TEMPLATE'])

        for var_info in self.c_dict['VAR_LIST']:
            level = var_info['fcst_level']
            self.add_field_info_to_time_info(time_info, var_info)

            for storm_id in storm_list:
                time_info['storm_id'] = storm_id
                # get the output directory where the series_analysis output
                # was written. Plots will be written to the same directory
                plot_input = do_string_sub(output_dir_template,
                                           **time_info)

                # Get the number of forecast tile files and the name of the
                # first and last in the list to be used in the -title
                num, beg, end = (
                    self.get_fcst_file_info(os.path.dirname(plot_input),
                                            storm_id)
                )
                if num is None:
                    self.logger.debug(f"Skipping plot for {storm_id}")
                    continue

                # Assemble the input file, output file, field string, and title
                for cur_stat in self.c_dict['STAT_LIST']:
                    plot_output = (f"{os.path.splitext(plot_input)[0]}_"
                                   f"{cur_stat}.ps")

                    time_info['num_leads'] = num
                    time_info['fcst_beg'] = beg
                    time_info['fcst_end'] = end
                    time_info['stat'] = cur_stat
                    self.plot_data_plane.c_dict['INPUT_TEMPLATE'] = plot_input
                    self.plot_data_plane.c_dict['OUTPUT_TEMPLATE'] = plot_output
                    self.plot_data_plane.c_dict['FIELD_NAME'] = f"series_cnt_{cur_stat}"
                    self.plot_data_plane.c_dict['FIELD_LEVEL'] = level
                    self.plot_data_plane.run_at_time_once(time_info)
                    self.all_commands.extend(self.plot_data_plane.all_commands)
                    self.plot_data_plane.all_commands.clear()

    def get_fcst_file_info(self, output_dir, storm_id):
        """! Get the number of all the gridded forecast n x m tile
            files for a given storm id and init time
            (that were created by extract_tiles). Determine the filename of the
            first and last files.  This information is used to create
            the title value to the -title opt in plot_data_plane.

            @param output_dir Directory containing ASCII list file of
             forecast files to process
            @param storm_id ID of storm to process
            @returns num, beg, end:  A tuple representing the number of
            forecast tile files, and the first and last file. If info cannot
            be parsed, return (None, None, None)
        """
        fcst_path = os.path.join(output_dir,
                                 f"{self.FCST_ASCII_FILE_PREFIX}{storm_id}")
        # read the file but skip the first line because it contains 'file_list'
        with open(fcst_path, 'r') as file_handle:
            files_of_interest = file_handle.readlines()[1:]

        # Get a sorted list of the forecast tile files for the init
        # time of interest for all the storm ids and return the
        # forecast hour corresponding to the first and last file.
        sorted_files = sorted(files_of_interest)
        if not files_of_interest:
            self.log_error(f"No files found in file list: {fcst_path}")
            return None, None, None

        first = sorted_files[0]
        last = sorted_files[-1]

        # Extract the forecast hour from the first and last filenames.
        fcst_regex = f".*{self.c_dict['FCST_TILE_PREFIX']}" + "([0-9]{3}).*.nc"
        match_beg = re.search(fcst_regex, first)
        match_end = re.search(fcst_regex, last)

        if match_beg:
            beg = f"F{match_beg.group(1)}"
        else:
            self.log_error("Unexpected file format encountered, exiting...")
            return None, None, None

        if match_end:
            end = f"F{match_end.group(1)}"
        else:
            self.log_error("Unexpected file format encountered, exiting...")
            return None, None, None

        # Get the number of forecast tile files
        num = str(len(sorted_files))

        return num, beg, end
