'''! @namespace SeriesByInitWrapper
@brief Performs any optional filtering of input tcst data then performs
regridding via the MET tool regrid_data_plane, then builds up
the commands to perform a series analysis by init time by invoking the
MET tool series_analysis. NetCDF plots are generated by invoking the MET tool
plot_data_plane. The NetCDF plots are then converted to .png and Postscript.

Call as follows:
@code{.sh}
series_by_init.py [-c /path/to/user.template.conf]
@endcode
'''

import errno
import os
import re
import sys
from datetime import datetime

from ..util import met_util as util
from .tc_stat_wrapper import TCStatWrapper
from ..util import feature_util
from . import CommandBuilder

class SeriesByInitWrapper(CommandBuilder):
    """!  Performs series analysis based on init time by first performing any
          additional filtering via the wrapper to the MET tool tc_stat,
          tc_stat_wrapper.  Next, the arguments to run the MET tool
          series_analysis is done
    """

    def __init__(self, config):
        self.app_path = os.path.join(config.getdir('MET_BIN_DIR', ''),
                                     'series_analysis')
        self.app_name = os.path.basename(self.app_path)
        super().__init__(config)
        # Retrieve any necessary values (dirs, executables)
        # from the param file(s)
        self.stat_list = util.getlist(self.config.getstr('config', 'SERIES_ANALYSIS_STAT_LIST'))
        self.extract_tiles_dir = self.config.getdir('SERIES_ANALYSIS_INPUT_DIR')
        self.series_out_dir = self.config.getdir('SERIES_ANALYSIS_OUTPUT_DIR')
        self.series_filtered_out_dir = \
            self.config.getdir('SERIES_ANALYSIS_FILTERED_OUTPUT_DIR')
        self.filter_opts = \
            self.config.getstr('config', 'SERIES_ANALYSIS_FILTER_OPTS')
        self.fcst_ascii_file_prefix = 'FCST_ASCII_FILES_'
        self.anly_ascii_file_prefix = 'ANLY_ASCII_FILES_'
        self.convert_exe = self.config.getexe('CONVERT')
        if not self.convert_exe:
            self.isOK = False

        # Needed for generating plots
        self.sbi_plotting_out_dir = ''

        self.inaddons = []
        self.infiles = []
        self.outdir = ""
        self.outfile = ""
        self.args = []

        self.logger.info("Initialized SeriesByInitWrapper")

    def create_c_dict(self):
        c_dict = super().create_c_dict()
        c_dict['MODEL'] = self.config.getstr('config', 'MODEL', 'FCST')
        c_dict['REGRID_TO_GRID'] = self.config.getstr('config', 'SERIES_ANALYSIS_REGRID_TO_GRID', '')
        return c_dict

    def run_all_times(self):
        """! Invoke the series analysis script based on
            the init time in the format YYYYMMDD_hh

            Args:

            Returns:
                None:  Creates graphical plots of storm tracks
        """
        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.

        # Used for logging.
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name
        self.logger.info("Starting series analysis by init time")

        # Set up the environment variable to be used in the Series Analysis
        #   Config file (SERIES_ANALYSIS_BY_INIT_CONFIG_FILE)
        # Used to set cnt  value in output_stats in
        # "SERIES_ANALYSIS_BY_INIT_CONFIG_FILE"
        # Need to do some pre-processing so that Python will use " and not '
        #  because currently MET doesn't support single-quotes
        tmp_stat_string = str(self.stat_list)
        tmp_stat_string = tmp_stat_string.replace("\'", "\"")

        # For example, we want tmp_stat_string to look like
        #   '["TOTAL","FBAR"]', NOT "['TOTAL','FBAR']"
        os.environ['STAT_LIST'] = tmp_stat_string
        self.add_env_var('STAT_LIST', tmp_stat_string)

        series_filter_opts = \
            self.config.getstr('config', 'SERIES_ANALYSIS_FILTER_OPTS')

        # Regridding via MET Tool regrid_data_plane.
        fcst_tile_regex = self.config.getstr('regex_pattern',
                                             'FCST_SERIES_ANALYSIS_NC_TILE_REGEX')
        anly_tile_regex = self.config.getstr('regex_pattern',
                                             'OBS_SERIES_ANALYSIS_NC_TILE_REGEX')

        # Initialize the tile_dir to point to the extract_tiles_dir.
        # And retrieve a list of init times based on the data available in
        # the extract tiles directory.
        tile_dir = self.extract_tiles_dir
        init_times = util.get_updated_init_times(tile_dir, self.logger)

        # Check for input tile data.
        try:
            util.check_for_tiles(tile_dir, fcst_tile_regex,
                                 anly_tile_regex, self.logger)
        except OSError:
            msg = ("Missing n x m tile files.  " +
                   "Extract tiles needs to be run")
            self.log_error(msg)

        # If applicable, apply any filtering via tc_stat, as indicated in the
        # parameter/config file.
        staging_dir = self.config.getdir('STAGING_DIR')
        if series_filter_opts:
            self.apply_series_filters(tile_dir, init_times,
                                      self.series_filtered_out_dir,
                                      self.filter_opts,
                                      staging_dir)

            # Clean up any empty files and directories that could arise as
            # a result of filtering
            util.prune_empty(self.series_filtered_out_dir, self.logger)

            # Get the list of all the files that were created as a result
            # of applying the filter options.
            # First, make sure that the series_lead_filtered_out
            # directory isn't empty.  If so, then no files fall within the
            # filter criteria.
            if os.path.exists(self.series_filtered_out_dir) and \
                    os.listdir(self.series_filtered_out_dir):
                # The series filter directory has data, use this directory as
                # input for series analysis.
                tile_dir = self.series_filtered_out_dir

                # Generate the tmp_anly and tmp_fcst files used to validate
                # filtering and for troubleshooting
                # The tmp_fcst and tmp_anly ASCII files contain the
                # list of files that meet the filter criteria.
                filtered_dirs_list = util.get_files(tile_dir, ".*.",
                                                    self.logger)
                util.create_filter_tmp_files(filtered_dirs_list,
                                             self.series_filtered_out_dir,
                                             self.logger)

            else:
                # Applying the filter produced no results.  Rather than
                # stopping, continue by using the files from extract_
                # tiles as input.
                msg = ("Applied series filter options, no results..." +
                       "using extract tiles data for series analysis input.")
                self.logger.debug(msg)
                tile_dir = self.extract_tiles_dir

        else:
            # No additional filtering was requested.
            # Use the data in the extract tiles directory
            # as input for series analysis.
            # source of input tile data.
            tile_dir = self.extract_tiles_dir

        # Create FCST and ANLY ASCII files based on init time and storm id.
        # These are arguments to the
        # -fcst and -obs arguments to the MET Tool series_analysis.
        # First, get an updated list of init times,
        # since filtering can reduce the amount of init times.
        sorted_filter_init = self.get_ascii_storm_files_list(tile_dir)

        # Clean up any remaining empty files and dirs
        util.prune_empty(self.series_out_dir, self.logger)
        self.logger.debug("Finished creating FCST and ANLY ASCII files, and " +
                          "cleaning empty files and dirs")

        # Build up the arguments to and then run the MET tool series_analysis.
        self.build_and_run_series_request(sorted_filter_init, tile_dir)

        # Generate plots
        # Check for .nc files in output_dir first, if these are absent, the
        # there is a problem.
        if self.is_netcdf_created():
            self.generate_plots(sorted_filter_init, tile_dir)
        else:
            self.log_error("No NetCDF files were created by"
                              " series_analysis, exiting...")
            sys.exit(errno.ENODATA)

        # clean up the tmp dir now that we are finished and create a new empty
        # tmp dir
        filtered_file_regex = "filter_.*"
        util.remove_staged_files(staging_dir, filtered_file_regex, self.logger )

        self.logger.info("Finished series analysis by init time")

    def apply_series_filters(self, tile_dir, init_times, series_output_dir,
                             filter_opts, staging_dir):

        """! Apply filter options, as specified in the
            param/config file.
            Args:
               @param tile_dir:  Directory where input data files reside.
                                 e.g. data which we will be applying our filter
                                 criteria.
               @param init_times:  List of init times that define the
                                   input data.
               @param series_output_dir:  The directory where the filter results
                                          will be stored.
               @param filter_opts:  The filter options to apply
               @param staging_dir:  The staging directory where intermediate
                                      files are saved.
            Returns:
                None
        """
        # pylint: disable=too-many-arguments
        # Seven input arguments are needed to perform filtering.

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.

        # Useful for logging
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        # Create temporary directory where intermediate files are saved.
        self.logger.debug("creating tmp dir for filtered files: " + staging_dir)

        for cur_init in init_times:
            # Call the tc_stat wrapper to build up the command and invoke
            # the MET tool tc_stat.
            filter_file = "filter_" + cur_init + ".tcst"
            filter_filename = os.path.join(series_output_dir,
                                           cur_init, filter_file)

            input_dict = {'init': datetime.strptime(cur_init, '%Y%m%d_%H')}
            job_args = (f'-job filter {filter_opts}'
                        f' -dump_row {filter_filename}')
            override_dict = {'TC_STAT_JOB_ARGS': job_args,
                             'TC_STAT_INIT_INCLUDE': cur_init,
                             'TC_STAT_LOOKIN_DIR': tile_dir,
                             'TC_STAT_OUTPUT_DIR': series_output_dir,
                             'TC_STAT_MATCH_POINTS': True,
                             }
            tc_stat_wrapper = TCStatWrapper(self.config, override_dict)
            if not tc_stat_wrapper.isOK:
                continue

            if not tc_stat_wrapper.run_at_time(input_dict):
                continue

#            tcs = TCStatWrapper(self.config)
#            tcs.build_tc_stat(series_output_dir, cur_init, tile_dir,
#                              filter_opts)

            # Check that the filter.tcst file isn't empty. If
            # it is, then use the files from extract_tiles as
            # input (tile_dir = extract_out_dir)
            if not util.file_exists(filter_filename):
                msg = ("Non-existent filter file, filter " +
                       " Never created by MET Tool tc_stat.")
                self.logger.debug(msg)
                continue
            elif os.stat(filter_filename).st_size == 0:
                msg = ("Empty filter file, filter " +
                       " options yield nothing.")
                self.logger.debug(msg)
                continue
            else:
                # Now retrieve the files corresponding to these
                # storm ids that resulted from filtering.
                sorted_storm_ids = util.get_storm_ids(filter_filename,
                                                      self.logger)

                # Retrieve the header from filter_filename to be used in
                # creating the temporary files.
                with open(filter_filename, 'r') as filter_file:
                    header = filter_file.readline()

                for cur_storm in sorted_storm_ids:
                    msg = ("Processing storm: " +
                           cur_storm + " for file: " + filter_filename)
                    self.logger.debug(msg)
                    storm_output_dir = os.path.join(series_output_dir,
                                                    cur_init, cur_storm)
                    util.mkdir_p(storm_output_dir)
                    util.mkdir_p(staging_dir)
                    tmp_file = "filter_" + cur_init + "_" + cur_storm
                    tmp_filename = os.path.join(staging_dir, tmp_file)
                    storm_match_list = util.grep(cur_storm, filter_filename)
                    with open(tmp_filename, "w") as tmp_file:
                        tmp_file.write(header)
                        for storm_match in storm_match_list:
                            tmp_file.write(storm_match)

                    # Create the analysis and forecast files based
                    # on the storms (defined in the tmp_filename created above)
                    # Store the analysis and forecast files in the
                    # series_output_dir.
                    feature_util.retrieve_and_regrid(tmp_filename, cur_init,
                                                     cur_storm,
                                                     series_output_dir,
                                                     self.config)

                    # remove temp file
                    os.remove(tmp_filename)

        # Check for any empty files and directories and remove them to avoid
        # any errors or performance degradation when performing
        # series analysis.
        util.prune_empty(series_output_dir, self.logger)

    def is_netcdf_created(self):
        """! Check for the presence of NetCDF files in the series_analysis_init
             directory

             Returns:
                 is_created         True if NetCDF files were found in the
                                    series_analysis_init/YYYYMMDD_hh/storm
                                    sub-directories, False otherwise.
        """
        dated_dir_list = os.listdir(self.series_out_dir)
        netcdf_file_counter = 0
        is_created = False

        # Get the storm sub-directories in each dated sub-directory
        for dated_dir in dated_dir_list:
            dated_dir_path = os.path.join(self.series_out_dir, dated_dir)
            # Get a list of the storm sub-dirs in this directory
            all_storm_list = os.listdir(dated_dir_path)
            for each_storm in all_storm_list:
                full_storm_dirname = os.path.join(dated_dir_path, each_storm)
                # Now get the list of files for each storm sub-dir.
                all_files = os.listdir(full_storm_dirname)
                for each_file in all_files:
                    full_filepath = os.path.join(full_storm_dirname, each_file)
                    if os.path.isfile(full_filepath):
                        if full_filepath.endswith('.nc'):
                            netcdf_file_counter += 1

        if netcdf_file_counter > 0:
            is_created = True

        return is_created

    def get_fcst_file_info(self, dir_to_search, cur_init, cur_storm):
        """! Get the number of all the gridded forecast n x m tile
            files for a given storm id and init time
            (that were created by extract_tiles). Determine the filename of the
            first and last files.  This information is used to create
            the title value to the -title opt in plot_data_plane.

            Args:
            @param dir_to_search: The directory of the gridded files of
                                  interest.
            @param cur_init:  The init time of interest.
            @param cur_storm:  The storm id of interest.

            Returns:
            num, beg, end:  A tuple representing the number of
                            forecast tile files, and the first and
                            last file.

                            sys.exit(1) otherwise
        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        # For logging
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        # Get a sorted list of the forecast tile files for the init
        # time of interest for all the storm ids and return the
        # forecast hour corresponding to the first and last file.
        # base_dir_to_search = os.path.join(output_dir, cur_init)
        gridded_dir = os.path.join(dir_to_search, cur_init, cur_storm)
        search_regex = ".*FCST_TILE.*.nc"

        files_of_interest = util.get_files(gridded_dir, search_regex,
                                           self.logger)
        sorted_files = sorted(files_of_interest)
        if not files_of_interest:
            msg = ("exiting, no files found for " +
                   "init time of interest" +
                   " and directory:" + dir_to_search)
            self.log_error(msg)
            sys.exit(1)

        first = sorted_files[0]
        last = sorted_files[-1]

        # Extract the forecast hour from the first and last
        # filenames.
        match_beg = re.search(".*FCST_TILE_(F[0-9]{3}).*.nc", first)
        match_end = re.search(".*FCST_TILE_(F[0-9]{3}).*.nc", last)

        if match_beg:
            beg = match_beg.group(1)
        else:
            msg = ("Unexpected file format encountered, exiting...")
            self.log_error(msg)
            sys.exit(1)

        if match_end:
            end = match_end.group(1)
        else:
            msg = ("Unexpected file format encountered, exiting...")
            self.log_error(msg)
            sys.exit(1)

        # Get the number of forecast tile files
        num = len(sorted_files)

        return num, beg, end

    def get_ascii_storm_files_list(self, tile_dir):
        """! Creates the list of ASCII files that contain the storm id and init
             times.  The list is used to create an ASCII file which will be
             used as the option to the -obs or -fcst flag to the MET
             series_analysis tool.
             Args:
                   @param tile_dir:  The directory where input files reside.
             Returns:
                   sorted_filter_init:  A list of the sorted directories
                                        corresponding to the init times after
                                        filtering has been applied.  If
                                        filtering produced no results, this
                                        is the list of files created from
                                        running extract_tiles.
        """
        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        # For logging
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name
        filter_init_times = util.get_updated_init_times(tile_dir, self.logger)
        sorted_filter_init = sorted(filter_init_times)

        for cur_init in sorted_filter_init:
            # Get all the storm ids for storm track pairs that
            # correspond to this init time.
            storm_list = self.get_storms_for_init(cur_init, tile_dir)
            if not storm_list:
                # No storms for this init time,
                # check next init time in list
                continue
            else:
                for cur_storm in storm_list:
                    # First get the filenames for the gridded forecast and
                    # analysis (n deg x m deg tiles that were created by
                    # extract_tiles). These files are aggregated by
                    # init time and storm id.
                    anly_grid_regex = ".*ANLY_TILE_F.*nc"
                    fcst_grid_regex = ".*FCST_TILE_F.*nc"

                    anly_grid_files = util.get_files(tile_dir,
                                                     anly_grid_regex,
                                                     self.logger)
                    fcst_grid_files = util.get_files(tile_dir,
                                                     fcst_grid_regex,
                                                     self.logger)

                    # Now do some checking to make sure we aren't
                    # missing either the forecast or
                    # analysis files, if so log the error and proceed to next
                    # storm in the list.
                    if not anly_grid_files or not fcst_grid_files:
                        # No gridded analysis or forecast
                        # files found, continue
                        self.logger.info("no gridded analysis or forecast " +
                                         "file found, continue to next storm")
                        continue

                    # Now create the FCST and ANLY ASCII files based on
                    # cur_init and cur_storm:
                    self.create_fcst_anly_to_ascii_file(
                        fcst_grid_files, cur_init, cur_storm,
                        self.fcst_ascii_file_prefix)
                    self.create_fcst_anly_to_ascii_file(
                        anly_grid_files, cur_init, cur_storm,
                        self.anly_ascii_file_prefix)
                    util.prune_empty(self.series_out_dir, self.logger)
        return sorted_filter_init

    def build_and_run_series_request(self, sorted_filter_init, tile_dir):
        """! Build up the -obs, -fcst, -out necessary for running the
             series_analysis MET tool, then invoke series_analysis.

             Args:
                  @param sorted_filter_init:  A list of the sorted directories
                                        corresponding to the init times that
                                        are the result of filtering.  If
                                        filtering produced no results, this
                                        is the list of files created from
                                        running extract_tiles.
                  @param tile_dir:  The directory where the input resides.
             Returns:
        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        # For logging
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name


        # Now assemble the -fcst, -obs, and -out arguments and invoke the
        # MET Tool: series_analysis.
        for cur_init in sorted_filter_init:
            storm_list = self.get_storms_for_init(cur_init, tile_dir)
            for cur_storm in storm_list:
                if not storm_list:
                    # No storm ids found for cur_init
                    # check next init time in the list.
                    continue
                else:
                    # Build the -obs and -fcst portions of the series_analysis
                    # command. Then generate the -out portion, get the NAME and
                    # corresponding LEVEL for each variable.
                    full_vars_list = feature_util.retrieve_var_name_levels(self.config)
                    for cur_var in full_vars_list:
                        name, level = cur_var
                        param = \
                            self.config.getstr(
                                'config',
                                'SERIES_ANALYSIS_CONFIG_FILE')
                        self.param = param
                        self.create_obs_fcst_arg('obs',
                                                 self.anly_ascii_file_prefix,
                                                 cur_storm, cur_init)
                        self.create_obs_fcst_arg('fcst',
                                                 self.fcst_ascii_file_prefix,
                                                 cur_storm,
                                                 cur_init)
                        self.create_out_arg(cur_storm, cur_init, name, level)
                        self.add_common_envs()
                        super().set_environment_variables()
                        self.build()
                        self.clear()

    def create_obs_fcst_arg(self, param_arg, ascii_file_base, cur_storm,
                            cur_init):
        """! Create the argument to the -obs or -fcst flag to the MET tool,
             series_analysis.

             Args:
                 @param param_arg:  '-obs' for the -obs argument to
                                    series_analysis, or '-fcst' for the
                                    -fcst argument to series_analysis.
                 @param ascii_file_base:  The base name of the obs or fcst
                                          ASCII file that contains a list
                                          of the full filepath to the input
                                          files.

                 @param cur_storm:  The storm of interest.

                 @param cur_init:   The initialization time of interest.

             Returns:
        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        # For logging
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name
        ascii_fname_parts = [ascii_file_base, cur_storm]
        ascii_fname = ''.join(ascii_fname_parts)
        ascii_full_path = os.path.join(self.series_out_dir, cur_init,
                                       cur_storm, ascii_fname)
        self.add_input_file(ascii_full_path, param_arg)
        latest_idx = len(self.infiles) - 1
        self.logger.debug("first param: " + self.infiles[latest_idx])

    def create_out_arg(self, cur_storm, cur_init, name, level):
        """! Create/build the -out portion of the series_analysis command and
             creates the output directory.
            Args:
                @param cur_storm: The storm of interest.

                @param cur_init:  The initialization time of interest.

                @param name:  The variable name of interest.

                @param level:  The level of interest.

            Returns:
        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        # For logging
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        # create the output dir
        self.outdir = os.path.join(self.series_out_dir, cur_init,
                                   cur_storm)
        util.mkdir_p(self.outdir)
        # Set the NAME and LEVEL environment variables, this
        # is required by the MET series_analysis binary.
        os.environ['NAME'] = name
        os.environ['LEVEL'] = level
        self.add_env_var('NAME', name)
        self.add_env_var('LEVEL', level)

        series_anly_output_parts = [self.outdir, '/',
                                    'series_', name, '_',
                                    level, '.nc']
        # Set the sbi_out_dir for this instance, this will be
        # used for generating the plot.
        self.sbi_plotting_out_dir = ''.join(
            series_anly_output_parts)
        self.outfile = self.sbi_plotting_out_dir

        self.logger.debug('output arg/output dir for series_analysis: ' +
                          self.get_output_path())

    def clear(self):
        super().clear()
        self.inaddons = []

    def add_input_file(self, filename, type_id):
        self.infiles.append(filename)
        self.inaddons.append("-" + type_id)

    def get_command(self):
        if self.app_path is None:
            self.log_error("No app path specified. You must use a subclass")
            return None

        cmd = self.app_path + " "
        if self.args:
            for a in self.args:
                cmd += a + " "

        for idx, f in enumerate(self.infiles):
            cmd += self.inaddons[idx] + " " + f + " "

        if self.param != "":
            cmd += "-config " + self.param + " "

        if self.get_output_path() == "":
            self.logger.info("No output directory specified, because series analysis has multiple directories")
            self.logger.info("No output filename specified, because series analysis has multiple files")
        else:
            cmd += "-out " + os.path.join(self.get_output_path())

        return cmd

    def generate_plots(self, sorted_filter_init, tile_dir):
        """! Generate the plots from the series_analysis output.
           Args:
               @param sorted_filter_init:  A list of the sorted directories
                                        corresponding to the init times (that
                                        are the result of filtering).  If
                                        filtering produced no results, this
                                        is the list of files created from
                                        running extract_tiles.

               @param tile_dir:  The directory where input data resides.
           Returns:
        """
        background_map = self.config.getbool('config', 'SERIES_ANALYSIS_BACKGROUND_MAP')
        plot_data_plane_exe = os.path.join(self.config.getdir('MET_BIN_DIR', ''),
                                           'plot_data_plane')

        full_vars_list = feature_util.retrieve_var_name_levels(self.config)
        for cur_var in full_vars_list:
            name, level = cur_var
            for cur_init in sorted_filter_init:
                storm_list = self.get_storms_for_init(cur_init, tile_dir)
                for cur_storm in storm_list:
                    # create the output directory where the finished
                    # plots will reside
                    output_dir = os.path.join(self.series_out_dir, cur_init,
                                              cur_storm)
                    util.mkdir_p(output_dir)

                    # Now we need to invoke the MET tool
                    # plot_data_plane to generate plots that are
                    # recognized by the MET viewer.
                    # Get the number of forecast tile files,
                    # the name of the first and last in the list
                    # to be used by the -title option.
                    if tile_dir == self.extract_tiles_dir:
                        # Since filtering was not requested, or
                        # the additional filtering doesn't yield results,
                        # search the series_out_dir
                        num, beg, end = \
                            self.get_fcst_file_info(self.series_out_dir,
                                                    cur_init, cur_storm)
                    else:
                        # Search the series_filtered_out_dir for
                        # the filtered files.
                        num, beg, end = self.get_fcst_file_info(
                            self.series_filtered_out_dir, cur_init,
                            cur_storm)

                    # Assemble the input file, output file, field string,
                    # and title
                    plot_data_plane_input_fname = self.sbi_plotting_out_dir
                    for cur_stat in self.stat_list:
                        plot_data_plane_output = [output_dir,
                                                  '/series_',
                                                  name, '_',
                                                  level, '_',
                                                  cur_stat, '.ps']
                        plot_data_plane_output_fname = ''.join(
                            plot_data_plane_output)
                        os.environ['CUR_STAT'] = cur_stat
                        self.add_env_var('CUR_STAT', cur_stat)

                        # Create versions of the arg based on
                        # whether the background map is requested
                        # in param file.
                        map_data = ' map_data={ source=[];}'

                        if background_map:
                            # Flag set to True, draw background map.
                            field_string_parts = ["'name=", '"series_cnt_',
                                                  cur_stat, '";',
                                                  'level="', level, '";',
                                                  "'"]
                        else:
                            field_string_parts = ["'name=", '"series_cnt_',
                                                  cur_stat, '";',
                                                  'level="', level, '";',
                                                  map_data, "'"]

                        field_string = ''.join(field_string_parts)
                        title_parts = [' -title "GFS Init ', cur_init,
                                       ' Storm ', cur_storm, ' ',
                                       str(num), ' Forecasts (',
                                       str(beg), ' to ', str(end),
                                       '),', cur_stat, ' for ',
                                       name, ', ',  level, '"']
                        title = ''.join(title_parts)

                        # Now assemble the entire plot data plane command
                        data_plane_command_parts = \
                            [plot_data_plane_exe, ' ',
                             plot_data_plane_input_fname, ' ',
                             plot_data_plane_output_fname, ' ',
                             field_string, ' ', title]

                        data_plane_command = ''.join(
                            data_plane_command_parts)

                        # Since this wrapper is not using the CommandBuilder
                        # to build the cmd, we need to add the met verbosity
                        # level to the MET cmd created before we run
                        # the command.
                        data_plane_command = self.cmdrunner.insert_metverbosity_opt\
                            (data_plane_command)
                        (ret, cmd) = self.cmdrunner.run_cmd\
                            (data_plane_command, env=None, app_name=self.app_name)

                        if ret != 0:
                            self.log_error(f"MET command returned a non-zero return code: {cmd}")
                            self.logger.info("Check the logfile for more information on why it failed")

                        # Now assemble the command to convert the
                        # postscript file to png
                        png_fname = plot_data_plane_output_fname.replace(
                            '.ps', '.png')
                        convert_parts = [self.convert_exe, ' -rotate 90',
                                         ' -background white -flatten ',
                                         plot_data_plane_output_fname,
                                         ' ', png_fname]
                        convert_command = ''.join(convert_parts)

                        (ret, cmd) = self.cmdrunner.run_cmd(convert_command, ismetcmd=False)
                        if ret != 0:
                            self.log_error(f"MET command returned a non-zero return code: {cmd}")
                            self.logger.info("Check the logfile for more information on why it failed")

    def get_storms_for_init(self, cur_init, out_dir_base):
        """! Retrieve all the filter files which have the .tcst
             extension.  Inside each file, extract the STORM_ID
             and append to the list, if the storm_list directory
             exists.

            Args:
              @param cur_init: the init time

              @param out_dir_base:  The directory where one should start
                                    searching for the filter file(s)
                                    - those with a .tcst file extension.


        Returns:
            storm_list: A list of all the storms ids that correspond to
                        this init time and actually has a directory in the
                        init dir (additional filtering in a previous step
                        may result in missing storm ids even though they are
                        in the filter.tcst file)
        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        # For logging
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        # Retrieve filter files, first create the filename
        # by piecing together the out_dir_base with the cur_init.
        filter_filename = "filter_" + cur_init + ".tcst"
        filter_file = os.path.join(out_dir_base, cur_init, filter_filename)

        # Now that we have the filter filename for the init time, let's
        # extract all the storm ids in this filter file.
        storm_list = util.get_storm_ids(filter_file, self.logger)

        return storm_list

    def create_fcst_anly_to_ascii_file(self, fcst_anly_grid_files, cur_init,
                                       cur_storm, fcst_anly_filename_base):
        """! Create ASCII file for either the FCST or ANLY files that are
             aggregated based on init time and storm id.

        Args:
            fcst_anly_grid_files:       A list of the FCST or ANLY gridded
                                        files under consideration.

            cur_init:                  The initialization time of interest

            cur_storm:                 The storm id of interest

            fcst_anly_filename_base:   The base name of the ASCII file
                                        (either ANLY_ASCII_FILES_ or
                                        FCST_ASCII_FILES_ which will be
                                        appended with the storm id.

        Returns:
            None:                      Creates an ASCII file containing a list
                                        of either FCST or ANLY files based on
                                        init time and storm id.
        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.

        # For logging
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        # Create an ASCII file containing a list of all
        # the fcst or analysis tiles.
        fcst_anly_ascii_fname_parts = [fcst_anly_filename_base, cur_storm]
        fcst_anly_ascii_fname = ''.join(fcst_anly_ascii_fname_parts)
        fcst_anly_ascii_dir = os.path.join(self.series_out_dir, cur_init,
                                           cur_storm)
        util.mkdir_p(fcst_anly_ascii_dir)
        fcst_anly_ascii = os.path.join(fcst_anly_ascii_dir,
                                       fcst_anly_ascii_fname)

        # Sort the files in the fcst_anly_grid_files list.
        sorted_fcst_anly_grid_files = sorted(fcst_anly_grid_files)
        tmp_param = ''
        for cur_fcst_anly in sorted_fcst_anly_grid_files:
            # Write out the files that pertain to this storm and
            # don't write if already in tmp_param.
            if cur_fcst_anly not in tmp_param and cur_storm in cur_fcst_anly:
                tmp_param += cur_fcst_anly
                tmp_param += '\n'
        # Now create the fcst or analysis ASCII file
        try:
            with open(fcst_anly_ascii, 'w') as filehandle:
                filehandle.write(tmp_param)
        except IOError:
            msg = ("Could not create requested ASCII file:  " +
                   fcst_anly_ascii)
            self.log_error(msg)

        if os.stat(fcst_anly_ascii).st_size == 0:
            # Just in case there are any empty fcst ASCII or anly ASCII files
            # at this point,
            # explicitly remove them (and any resulting empty directories)
            #  so they don't cause any problems with further processing
            # steps.
            util.prune_empty(fcst_anly_ascii_dir, self.logger)
