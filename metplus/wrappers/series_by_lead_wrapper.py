import re
import os
import sys
import errno
import glob
from datetime import datetime

from ..util import met_util as util
from ..util import time_util
from ..util import feature_util
from . import CommandBuilder
from .tc_stat_wrapper import TCStatWrapper
from . import RegridDataPlaneWrapper

## @namespace SeriesByLeadWrapper
# @brief Performs any optional filtering of input tcst data then performs
# regridding via either MET regrid_data_plane or wgrib2, then builds up
# the commands to perform a series analysis by lead time by invoking the
# MET tool series_analysis. NetCDF plots are generated by invoking the MET tool
# plot_data_plane. The NetCDF plots are then converted to .png and Postscript,
# and an animated GIF representative of the entire series is generated.
#
# Call as follows:
# @code{.sh}
# SeriesByLeadWrapper.py [-c /path/to/user.template.conf]
# @endcode

# pylint:disable=too-many-instance-attributes
# all the attributes are necessary for performing tasks.
class SeriesByLeadWrapper(CommandBuilder):
    """! @brief SeriesByLeadWrapper performs series analysis of paired
         data based on lead time and generates plots for each requested
         variable and statistic, as specified in a configuration/parameter
         file.
    """

    def __init__(self, config, instance=None, config_overrides={}):
        self.app_name = 'series_analysis'
        super().__init__(config,
                         instance=instance,
                         config_overrides=config_overrides)
        # Retrieve any necessary values from the parm file(s)
        self.do_fhr_by_group = self.config.getbool('config',
                                                   'SERIES_ANALYSIS_GROUP_FCSTS')
        self.fhr_group_labels = []
        self.stat_list = util.getlist(self.config.getstr('config', 'SERIES_ANALYSIS_STAT_LIST'))
        self.plot_data_plane_exe = os.path.join(
            self.config.getdir('MET_BIN_DIR', ''),
            'plot_data_plane')

        self.convert_exe = self.config.getexe('CONVERT')
        self.ncap2_exe = self.config.getexe('NCAP2')
        self.ncdump_exe = self.config.getexe('NCDUMP')
        self.rm_exe = self.config.getexe("RM")
        if not self.convert_exe or not self.ncap2_exe or not self.ncdump_exe or not self.rm_exe:
            self.isOK = False

        met_bin_dir = self.config.getdir('MET_BIN_DIR', '')
        self.series_analysis_exe = os.path.join(met_bin_dir,
                                                'series_analysis')
        self.input_dir = self.config.getdir('SERIES_ANALYSIS_INPUT_DIR')
        self.series_lead_filtered_out_dir = \
            self.config.getdir('SERIES_ANALYSIS_FILTERED_OUTPUT_DIR')
        self.series_lead_out_dir = self.config.getdir('SERIES_ANALYSIS_OUTPUT_DIR')
        self.staging_dir = self.config.getdir('STAGING_DIR')
        self.background_map = self.config.getbool('config', 'SERIES_ANALYSIS_BACKGROUND_MAP')
        self.series_filter_opts = \
            self.config.getstr('config', 'SERIES_ANALYSIS_FILTER_OPTS')
        self.series_filter_opts.strip()
        self.fcst_ascii_regex = \
            self.config.getstr('regex_pattern', 'FCST_SERIES_ANALYSIS_ASCII_REGEX_LEAD')
        self.anly_ascii_regex = \
            self.config.getstr('regex_pattern', 'OBS_SERIES_ANALYSIS_ASCII_REGEX_LEAD')
        self.series_anly_configuration_file = \
            self.config.getstr('config', 'SERIES_ANALYSIS_CONFIG_FILE')

        # Re-gridding via MET Tool regrid_data_plane.
        self.fcst_tile_regex = \
            self.config.getstr('regex_pattern', 'FCST_SERIES_ANALYSIS_NC_TILE_REGEX')
        self.anly_tile_regex = \
            self.config.getstr('regex_pattern', 'OBS_SERIES_ANALYSIS_NC_TILE_REGEX')

        self.logger.info("Initialized SeriesByLeadWrapper")

    def create_c_dict(self):
        c_dict = super().create_c_dict()
        c_dict['MODEL'] = self.config.getstr('config', 'MODEL', 'FCST')
        c_dict['REGRID_TO_GRID'] = self.config.getstr('config', 'SERIES_ANALYSIS_REGRID_TO_GRID', '')
        return c_dict

    def get_lead_sequences(self):
        # output will be a dictionary where the key will be the
        #  label specified and the value will be the list of forecast leads
        lead_seq_dict = {}
        # used in plotting
        self.fhr_group_labels = []
        all_conf = self.config.keys('config')
        indices = []
        regex = re.compile(r"LEAD_SEQ_(\d+)")
        for conf in all_conf:
            result = regex.match(conf)
            if result is not None:
                indices.append(result.group(1))

        # loop over all possible variables and add them to list
        for n in indices:
            if self.config.has_option('config', "LEAD_SEQ_"+n+"_LABEL"):
                label = self.config.getstr('config', "LEAD_SEQ_"+n+"_LABEL")
            else:
                log_msg = 'Need to set LEAD_SEQ_{}_LABEL to describe ' +\
                          'LEAD_SEQ_{}'.format(n, n)
                self.log_error(log_msg)
                exit(1)

            # get forecast list for n
            lead_seq = util.getlistint(self.config.getstr('config', 'LEAD_SEQ_'+n))

            # add to output dictionary
            lead_seq_dict[label] = lead_seq

            self.fhr_group_labels.append(label)

        return lead_seq_dict


    def run_all_times(self):
        """! Perform a series analysis of extra tropical cyclone
             paired data based on lead time (forecast hour)
             This requires invoking the MET run_series_analysis binary,
             followed by generating graphics that are recognized by
             the MET viewer using the plot_data_plane and converting to
             postscript.
             A pre-requisite is the presence of the filter file and storm files
             (set to nxm degree tiles as indicated in the param/config file)
             the specified init and lead times.

             Create the following command to satisfy MET series_analysis:
             series_analysis -fcst <FILTERED_OUT_DIR>/FCST_FILES_F<CUR_FHR>
                         -obs <FILTERED_OUT_DIR>/ANLY_FILES_F<CUR_FHR>
                         -out <OUT_DIR>/series_F<CURR_FHR_<NAME>_<LEVEL>.nc
                         -config SeriesAnalysisConfig_by_lead
            Args:

            Returns:
                None:   Invokes MET series_analysis and any other MET
                        tool to perform series analysis.  Then plots
                        are generated for the variables and statistics (as
                        indicated in the param/config file) corresponding to
                        each forecast lead time.
    """

        # Flag used to determine whether to use the forecast hour range and
        # increment, or the specified list of forecast hours in creating the
        # Series-analysis command.
        do_fhr_by_range = not self.do_fhr_by_group

        # Set up the environment variable to be used in the Series Analysis
        #   Config file (SERIES_ANALYSIS_BY_LEAD_CONFIG_FILE)
        # Used to set cnt value in output_stats in
        # "SERIES_ANALYSIS_BY_LEAD_CONFIG_FILE"
        # Need to do some pre-processing so that Python will use " and not '
        #  because currently MET doesn't support single-quotes
        tmp_stat_string = str(self.stat_list)
        tmp_stat_string = tmp_stat_string.replace("\'", "\"")

        # For example, we want tmp_stat_string to look like
        #   '["TOTAL","FBAR"]', NOT "['TOTAL','FBAR']"
        self.add_env_var('STAT_LIST', tmp_stat_string)

        self.logger.info("Begin series analysis by lead...")

        # Initialize the tile_dir to the extract tiles output directory.
        # And retrieve a list of init times based on the data available in
        # the extract tiles directory.
        tile_dir = self.input_dir
        init_times = util.get_updated_init_times(tile_dir, self.logger)

        # Check for the existence of the storm track tiles and raise
        # an error if these are missing.
        try:
            util.check_for_tiles(tile_dir, self.fcst_tile_regex,
                                 self.anly_tile_regex, self.logger)
        except OSError:
            msg = ("Missing 30x30 tile files." +
                   "  Extract tiles needs to be run")
            self.log_error(msg)

        # Apply optional filtering via tc_stat, as indicated in the
        # parameter/config file.
        tile_dir = self.filter_with_tc_stat(tile_dir, init_times)

        if do_fhr_by_range:
            # entire range of forecast hours
            self.logger.debug("performing series analysis on entire range"
                              " of fhrs...")
            self.perform_series_for_all_fhrs(tile_dir)
        else:
            # Perform series analysis on groupings of forecast hours
            # specified in the METplus config file.
            self.logger.debug("performing series analysis on groupings of"
                              " forecast hours...")
            self.perform_series_for_fhr_groups(tile_dir)

        # Generate plots in NetCDF, png, and Postscript.
        self.generate_plots(do_fhr_by_range)

        # Create animated gif
        self.create_animated_gifs(do_fhr_by_range)

        self.logger.info("Finished with series analysis by lead")

    def filter_with_tc_stat(self, tile_dir, init_times):
        """! Perform optional filtering using MET tc_stat

            Args:
              @param tile_dir: The directory where the input data resides.
              @param init_times: A list of init times under which series
              filters will be applied.
            Returns:
              filter_tile_dir:  A directory of the resulting files from applying
                         the filter criteria (as specified in the param/
                         config file).
        """
        if self.series_filter_opts:
            util.mkdir_p(self.series_lead_filtered_out_dir)
            self.apply_series_filters(tile_dir, init_times,
                                      self.series_lead_filtered_out_dir,
                                      self.series_filter_opts,
                                      self.staging_dir)

            # Remove any empty files and directories to avoid
            # errors or performance degradation when performing
            # series analysis.
            util.prune_empty(self.series_lead_filtered_out_dir, self.logger)

            # Get the list of all the files that were created as a result
            # of applying the filter options.  Save this information, it
            # will be useful for troubleshooting and validating the correctness
            # of filtering.

            # First, make sure that the series_lead_filtered_out directory
            # isn't empty.  If so, then no files fall within the filter
            # criteria.
            if os.listdir(self.series_lead_filtered_out_dir):
                # Filtering produces results, assign the tile_dir to
                # the filter output directory, series_lead_filtered_out_dir.
                filtered_files_list = util.get_files(tile_dir, ".*.",
                                                     self.logger)

                # Create the tmp_fcst and tmp_anly ASCII files containing the
                # list of files that meet the filter criteria.
                util.create_filter_tmp_files(filtered_files_list,
                                             self.series_lead_filtered_out_dir)
                filter_tile_dir = self.series_lead_filtered_out_dir
            else:
                # No data meet filter criteria, use data from extract
                #  tiles directory.
                msg = ("After applying filter options, no data meet"
                       " filter criteria. Continue using all available data "
                       "in extract tiles directory.")
                self.logger.debug(msg)
                filter_tile_dir = self.input_dir

        else:
            # No additional filtering was requested.  The extract tiles
            # directory is the
            # source of input tile data.
            filter_tile_dir = self.input_dir

        return filter_tile_dir

    def perform_series_for_fhr_groups(self, tile_dir):
        """! Series analysis for groups based on forecast hours

              Args:
                  @param tile_dir:  The location where input data resides.
              Returns:       None

        """
        lead_seq_dict = self.get_lead_sequences()

        self.logger.debug(' Performing series analysis on forecast hour'
                          ' groupings.')

        # Create the output directory where the series analysis results
        # will be saved.
        #
        # Build up the arguments used to run MET series_analysis:
        # 1) Create groupings of the forecast gridded tiles and
        #    analysis gridded tiles so they can be saved in ASCII files to be
        #    used for the -fcst and -obs arguments.
        # 2) Build up the -out argument
        # 3) Create the full command for running MET series_analysis by
        #    combining the -fcst -obs, -out and other arguments.

        util.mkdir_p(self.series_lead_out_dir)

        for cur_label, lead_seq in lead_seq_dict.items():
            fcst_tiles_list = []
            anly_tiles_list = []
            cur_beg_str = str(lead_seq[0]).zfill(3)
            cur_end_str = str(lead_seq[-1]).zfill(3)

            out_dir_parts = [self.series_lead_out_dir, '/', cur_label]
            out_dir = ''.join(out_dir_parts)
            util.mkdir_p(out_dir)

            # Location of "grouped" FCST_FILES_Fhhh and ANLY_FILES_Fhhh
            ascii_fcst_file_parts = [out_dir, '/FCST_FILES_F',
                                     cur_beg_str + '_to_F' + cur_end_str]
            ascii_anly_file_parts = [out_dir, '/ANLY_FILES_F',
                                     cur_beg_str + '_to_F' + cur_end_str]
            ascii_fcst_file = ''.join(ascii_fcst_file_parts)
            ascii_anly_file = ''.join(ascii_anly_file_parts)

            msg = 'Evaluating forecast hours: {}'.format(lead_seq)
            self.logger.debug(msg)

            # Loop over each forecast hour within each group
            for cur_fhr in lead_seq:
                cur_fcst_tiles_list = self.get_anly_or_fcst_files(
                    tile_dir, "FCST", self.fcst_tile_regex,
                    cur_fhr)

                cur_anly_tiles_list = self.get_anly_or_fcst_files(
                    tile_dir, "ANLY", self.anly_tile_regex, cur_fhr)

                # Iterate over each forecast hour beg, end pair in this
                # grouping
                for cur_fcst in cur_fcst_tiles_list:
                    fcst_tiles_list.append(cur_fcst)

                for cur_anly in cur_anly_tiles_list:
                    anly_tiles_list.append(cur_anly)

            fcst_tiles_list = sorted(fcst_tiles_list)
            anly_tiles_list = sorted(anly_tiles_list)
            # Create the FCST and ANLY ASCII files that are the args
            # to the -fcst and -obs portion of the series_analysis
            # command.

            # For FCST
            try:
                if not fcst_tiles_list:
                    msg = (" No fcst_tiles for fhr group: " + cur_beg_str +
                            " to " + cur_end_str +
                            " Don't create FCST_F<fhr> ASCII file")
                    self.logger.debug(msg)
                else:
                    with open(ascii_fcst_file, 'w') as file_handle:
                        for fcst_tiles in fcst_tiles_list:
                            file_handle.write(fcst_tiles)
                            file_handle.write('\n')
            except IOError as io_error:
                msg = ("Could not create requested" +
                        " ASCII file: " + ascii_fcst_file + " | ")
                self.log_error(msg + io_error)

            # For ANLY
            try:
                if not anly_tiles_list:
                    msg = ("No anly_tiles for fhr group: " +
                            cur_beg_str + " to " + cur_end_str +
                            " Don't create ANLY_F<fhr> ASCII file")
                    self.logger.debug(msg)
                else:
                    with open(ascii_anly_file, 'w') as file_handle:
                        for anly_tiles in anly_tiles_list:
                            file_handle.write(anly_tiles)
                            file_handle.write('\n')

            except IOError as io_error:
                msg = ("Could not create requested" +
                        " ASCII file: " + ascii_anly_file + " | ")
                self.log_error(msg + io_error)

            # Remove any empty directories that were created when no
            # files were written.
            util.prune_empty(out_dir, self.logger)

            # Create the -fcst and -obs portion of the series_analysis
            # command.
            fcst_param_parts = ['-fcst ', ascii_fcst_file]
            fcst_param = ''.join(fcst_param_parts)
            obs_param_parts = ['-obs ', ascii_anly_file]
            obs_param = ''.join(obs_param_parts)
            self.logger.debug('fcst param: ' + fcst_param)
            self.logger.debug('obs param: ' + obs_param)

            # Create the -out portion of the series_analysis command.
            full_vars_list = feature_util.retrieve_var_name_levels(self.config)
            for cur_var in full_vars_list:
                name, level = cur_var
                self.add_env_var('LEVEL', level)

                # Set the NAME environment to <name>_<level> format if
                # regridding method is to be done with the MET tool
                # regrid_data_plane.
                self.add_env_var('NAME', name)
                out_param_parts = ['-out ', out_dir, '/series_F',
                                    cur_beg_str, '_to_F', cur_end_str,
                                    '_', name, '_', level, '.nc']
                out_param = ''.join(out_param_parts)

                # Create the full series analysis command.
                config_param_parts = ['-config ',
                                        self.series_anly_configuration_file]
                config_param = ''.join(config_param_parts)
                series_analysis_cmd_parts = [self.series_analysis_exe, ' ',
                                                fcst_param, ' ',
                                                obs_param, ' ', config_param,
                                                ' ', out_param]
                series_analysis_cmd = ''.join(series_analysis_cmd_parts)

                self.add_common_envs()
                super().set_environment_variables()

                # Since this wrapper is not using the CommandBuilder
                # to build the cmd, we need to add the met verbosity
                # level to the MET cmd created before we run
                # the command.
                series_analysis_cmd =\
                    self.cmdrunner.insert_metverbosity_opt \
                    (series_analysis_cmd)
                (ret, series_analysis_cmd) = self.cmdrunner.run_cmd \
                    (series_analysis_cmd, env=self.env, app_name='series_analysis')

                if ret != 0:
                    self.log_error(f"MET command returned a non-zero return code: {series_analysis_cmd}")
                    self.logger.info("Check the logfile for more information on why it failed")

                # Clean up any empty files and directories that still
                # persist.
                util.prune_empty(self.series_lead_out_dir, self.logger)


    def perform_series_for_all_fhrs(self, tile_dir):
        """! Performs a series analysis by lead time, based on a range and
             increment of forecast hours. Invokes the MET tool Series-analysis

             Args:
                   @param tile_dir:  The location of the input data (output
                                     from running ExtractTiles.py)
                   @param lead_seq:     List of forecast hours to process

             Returns:          None
        """
        # NOTE: SeriesByLead currently only supports hour leads
        # get_lead_sequence was modified to handle other lead units
        lead_seq = util.get_lead_sequence(self.config, None)

        for fhr in lead_seq:
            fcst_seconds = time_util.ti_get_seconds_from_relativedelta(fhr)
            if fcst_seconds is None:
                self.log_error(f'Invalid forecast units used: {fhr}')
                exit(1)

            cur_fhr = str(fcst_seconds // 3600).zfill(3)
            msg = ('Evaluating forecast hour ' + cur_fhr)
            self.logger.debug(msg)

            # Create the output directory where the netCDF series files
            # will be saved.
            util.mkdir_p(self.series_lead_out_dir)
            out_dir_parts = [self.series_lead_out_dir, '/', 'series_F',
                             cur_fhr]
            out_dir = ''.join(out_dir_parts)
            util.mkdir_p(out_dir)

            # Gather all the forecast gridded tile files
            # so they can be saved in ASCII files.
            fcst_tiles_list = self.get_anly_or_fcst_files(tile_dir, "FCST",
                                                          self.fcst_tile_regex,
                                                          cur_fhr)
            fcst_tiles_list = sorted(fcst_tiles_list)
            fcst_tiles = self.retrieve_fhr_tiles(fcst_tiles_list,
                                                 self.fcst_tile_regex)


            # Location of FCST_FILES_Fhhh
            ascii_fcst_file_parts = [out_dir, '/FCST_FILES_F', cur_fhr]
            ascii_fcst_file = ''.join(ascii_fcst_file_parts)

            # Now create the ASCII files needed for the -fcst and -obs
            try:
                if not fcst_tiles:
                    msg = ("No fcst_tiles for fhr: " +
                           cur_fhr + " Don't create FCST_F<fhr> ASCII file")
                    self.logger.debug(msg)
                    continue
                else:
                    with open(ascii_fcst_file, 'w') as file_handle:
                        file_handle.write(fcst_tiles)

            except IOError as io_error:
                msg = ("Could not create requested" +
                       " ASCII file: " + ascii_fcst_file + " | ")
                self.log_error(msg + io_error)

            # Gather all the anly gridded tile files
            # so they can be saved in ASCII files.
            anly_tiles_list = self.get_anly_or_fcst_files(tile_dir, "ANLY",
                                                          self.anly_tile_regex,
                                                          cur_fhr)
            anly_tiles_list = sorted(anly_tiles_list)
            anly_tiles = self.retrieve_fhr_tiles(anly_tiles_list,
                                                 self.anly_tile_regex)
            # Location of ANLY_FILES_Fhhh files
            # filtering.
            ascii_anly_file_parts = [out_dir, '/ANLY_FILES_F', cur_fhr]
            ascii_anly_file = ''.join(ascii_anly_file_parts)

            try:
                # Only write to the ascii_anly_file if
                # the anly_tiles string isn't empty.
                if not anly_tiles:
                    msg = ("No anly_tiles for fhr: " +
                           cur_fhr + " Don't create ANLY_F<fhr> ASCII file")
                    self.logger.debug(msg)
                    continue
                else:
                    with open(ascii_anly_file, 'w') as file_handle:
                        file_handle.write(anly_tiles)

            except IOError:
                self.log_error("Could not create requested " +
                                  "ASCII file: " + ascii_anly_file)

            # Remove any empty directories that result from
            # when no files are written.
            util.prune_empty(out_dir, self.logger)
            self.logger.debug("finished pruning empty files")

            # -fcst and -obs params
            fcst_param_parts = ['-fcst ', ascii_fcst_file]
            fcst_param = ''.join(fcst_param_parts)
            obs_param_parts = ['-obs ', ascii_anly_file]
            obs_param = ''.join(obs_param_parts)
            self.logger.debug('fcst param: ' + fcst_param)
            self.logger.debug('obs param: ' + obs_param)

            # Create the -out param and invoke the MET series
            # analysis binary
            # Get the name and level to create the -out param
            # and set the NAME and LEVEL environment variables that
            # are needed by the MET series analysis binary.
            full_vars_list = feature_util.retrieve_var_name_levels(self.config)
            for cur_var in full_vars_list:
                name, level = cur_var
                self.add_env_var('LEVEL', level)

                # Set NAME to name_level
                self.add_env_var('NAME', name)
                out_param_parts = ['-out ', out_dir, '/series_F', cur_fhr,
                                   '_', name, '_', level, '.nc']
                out_param = ''.join(out_param_parts)

                # Create the full series analysis command.
                config_param_parts = ['-config ',
                                      self.series_anly_configuration_file]
                config_param = ''.join(config_param_parts)
                series_analysis_cmd_parts = [self.series_analysis_exe, ' ',
                                             fcst_param, ' ', obs_param,
                                             ' ', config_param, ' ',
                                             out_param]
                series_analysis_cmd = ''.join(series_analysis_cmd_parts)

                self.add_common_envs()
                super().set_environment_variables()

                # Since this wrapper is not using the CommandBuilder
                # to build the cmd, we need to add the met verbosity
                # level to the MET cmd created before we run
                # the command.
                series_analysis_cmd = self.cmdrunner.insert_metverbosity_opt \
                    (series_analysis_cmd)
                (ret, series_analysis_cmd) = self.cmdrunner.run_cmd \
                    (series_analysis_cmd, env=self.env, app_name='series_analysis')

                if ret != 0:
                    self.log_error(f"MET command returned a non-zero return code: {series_analysis_cmd}")
                    self.logger.info("Check the logfile for more information on why it failed")

                # Make sure there aren't any emtpy
                # files or directories that still persist.
        util.prune_empty(self.series_lead_out_dir, self.logger)

    def get_nseries(self, do_fhr_by_range, nc_var_file):
        """! Determine the number of series for this lead time and
           its associated variable via calculating the max series_cnt_TOTAL
           value, maximum.

           Args:
              @param do_fhr_by_range:  Boolean value indicating whether series
                                analysis was performed on a range of forecast
                                hours (True) or on a "bucket" of forecast hours
                                (False).
              @param nc_var_file:  The netCDF file for a particular variable.

           Returns:
                 maximum (float): The maximum value of series_cnt_TOTAL of all
                                  the netCDF files for the variable cur_var.
                 None:          If no max value is found.
        """

        # Determine the series_F<fhr> subdirectory where this netCDF file
        # resides.
        if do_fhr_by_range:
            match = re.match(r'(.*/series_F[0-9]{3})/series_F[0-9]{3}.*nc',
                             nc_var_file)
        else:
            match = re.match(r'(.*/.*/)series_F[0-9]{3}_to_F[0-9]{3}.*nc',
                             nc_var_file)

        if match:
            base_nc_dir = match.group(1)
        else:
            msg = ("Cannot determine base directory path for " +
                   "netCDF files... exiting")
            self.log_error(msg)
            return None

        # TODO: critical. harden Do we ant to add -O overwrite option to
        # all ncap2 commands?? If I restart and not delete output
        # the user is prompted. This will pause any automation.

        # Use NCO utility ncap2 to find the max for the variable and
        # series_cnt_TOTAL pair.
        nseries_nc_path = os.path.join(base_nc_dir, 'nseries.nc')

        nco_nseries_cmd_parts = [self.ncap2_exe, ' -O -v -s ', '"',
                                 'max=max(series_cnt_TOTAL)', '" ',
                                 nc_var_file, ' ', nseries_nc_path]
        nco_nseries_cmd = ''.join(nco_nseries_cmd_parts)
        (ret, nco_nseries_cmd) = self.cmdrunner.run_cmd \
            (nco_nseries_cmd, env=self.env, ismetcmd=False)
        if ret:
            self.logger.debug("Could not read series_cnt_TOTAL "
                              f"from {nc_var_file}")
            return None

        # Create an ASCII file with the max value, which can be parsed.
        nseries_txt_path = os.path.join(base_nc_dir, 'nseries.txt')

        ncdump_max_cmd_parts = [self.ncdump_exe, ' ', nseries_nc_path,
                                '> ', nseries_txt_path]
        ncdump_max_cmd = ''.join(ncdump_max_cmd_parts)
        (ret, ncdump_max_cmd) = self.cmdrunner.run_cmd \
            (ncdump_max_cmd, env=self.env, ismetcmd=False, run_inshell=True)

        if ret:
            self.log_error("Command returned a non-zero return code:"
                           f"{ncdump_max_cmd}")
            self.logger.info("Check the logfile for more information "
                             "on why it failed")
            return None


        # Look for the max value for this netCDF file.
        try:
            with open(nseries_txt_path, 'r') as fmax:
                for line in fmax:
                    max_match = re.match(r'\s*max\s*=\s([-+]?\d*\.*\d*)',
                                         line)
                    if max_match:
                        maximum = max_match.group(1)

                        # Clean up any intermediate .nc and .txt files
                        nseries_list = [self.rm_exe + ' -rf', ' ', base_nc_dir,
                                        '/nseries.txt']
                        nseries_cmd = ''.join(nseries_list)
                        os.system(nseries_cmd)

                        return maximum
                    else:
                        # Remove the nseries.nc file, it is no longer needed
                        if os.path.isfile(nseries_nc_path):
                            self.logger.debug("REMOVING OLD nc path file")
                            os.remove(nseries_nc_path)

        except IOError:
            msg = ("cannot open the max text file")
            self.log_error(msg)

        return None

    def get_netcdf_min_max(self, do_fhr_by_range, nc_var_files, cur_stat):
        """! Determine the min and max for all lead times for each
           statistic and variable pairing.

           Args:
               @param do_fhr_by_range:  Boolean value indicating whether series
                                     analysis was performed on a range of
                                     forecast hours (True) or on a grouping
                                     of forecast hours (False).
               @param nc_var_files:  A list of the netCDF files generated
                                     by the MET series analysis tool that
                                     correspond to the variable of interest.
               @param cur_stat:      The current statistic of interest: ie.
                                     RMSE, MAE, ODEV, FDEV, ME, or TOTAL.

           Returns:
               tuple (vmin, vmax)
                   vmin:  The minimum
                   vmax:  The maximum

        """
        max_temporary_files = []
        min_temporary_files = []

        # Initialize the threshold values for min and max.
        vmin = 999999.
        vmax = -999999.

        for cur_nc in nc_var_files:
            # Determine the series_F<fhr> subdirectory where this
            # netCDF file resides.
            if do_fhr_by_range:
                match = re.match(r'(.*/series_F[0-9]{3})/series_F[0-9]{3}.*nc',
                                 cur_nc)
            else:
                match = re.match(r'(.*/.*)/'
                                 r'series_F[0-9]{3}_to_F[0-9]{3}.*nc', cur_nc)
            if match:
                base_nc_dir = match.group(1)
                self.logger.debug("base nc dir: " + base_nc_dir)
            else:
                msg = ("Cannot determine base directory path " +
                       "for netCDF files. Exiting...")
                self.log_error(msg)
                sys.exit(1)

            # Create file paths for temporary files for min value...
            min_nc_path = os.path.join(base_nc_dir, 'min.nc')
            min_txt_path = os.path.join(base_nc_dir, 'min.txt')
            min_temporary_files.append(min_nc_path)
            min_temporary_files.append(min_txt_path)

            # Clean up any temporary min files that might have been left over
            #  from a previous run.
            util.cleanup_temporary_files(min_temporary_files)

            # Use NCO ncap2 to get the min for the current stat-var pairing.
            nco_min_cmd_parts = [self.ncap2_exe, ' -O -v -s ', '"',
                                 'min=min(series_cnt_', cur_stat, ')',
                                 '" ', cur_nc, ' ', min_nc_path]
            nco_min_cmd = ''.join(nco_min_cmd_parts)
            self.logger.debug('nco_min_cmd: ' + nco_min_cmd)
            (ret, nco_min_cmd) = self.cmdrunner.run_cmd \
                (nco_min_cmd, env=self.env, ismetcmd=False)

            if ret != 0:
                self.log_error(f"Command returned a non-zero return code: {nco_min_cmd}")
                self.logger.info("Check the logfile for more information on why it failed")

            # now set up file paths for the max value...
            max_nc_path = os.path.join(base_nc_dir, 'max.nc')
            max_txt_path = os.path.join(base_nc_dir, 'max.txt')
            max_temporary_files.append(max_nc_path)
            max_temporary_files.append(max_txt_path)

            # First, remove pre-existing max.txt and max.nc file from any
            #  previous run.
            util.cleanup_temporary_files(max_temporary_files)

            # Using NCO ncap2 to perform arithmetic processing to retrieve
            #  the max from each
            # netCDF file's stat-var pairing.
            nco_max_cmd_parts = [self.ncap2_exe, ' -O -v -s ', '"',
                                 'max=max(series_cnt_', cur_stat, ')',
                                 '" ', cur_nc, ' ', max_nc_path]
            nco_max_cmd = ''.join(nco_max_cmd_parts)
            self.logger.debug('nco_max_cmd: ' + nco_max_cmd)
            (ret, nco_max_cmd) = self.cmdrunner.run_cmd(nco_max_cmd, env=self.env,
                                                        ismetcmd=False)
            if ret != 0:
                self.log_error(f"Command returned a non-zero return code: {nco_max_cmd}")
                self.logger.info("Check the logfile for more information on why it failed")

            # Create ASCII files with the min and max values, using the
            # NCO utility ncdump.
            # These files can be parsed to determine the vmin and vmax.
            ncdump_min_cmd_parts = [self.ncdump_exe, ' ', base_nc_dir,
                                    '/min.nc > ', min_txt_path]
            ncdump_min_cmd = ''.join(ncdump_min_cmd_parts)
            (ret, ncdump_min_cmd) = self.cmdrunner.run_cmd \
                (ncdump_min_cmd, env=self.env, ismetcmd=False, run_inshell=True)

            if ret != 0:
                self.log_error(f"Command returned a non-zero return code: {ncdump_min_cmd}")
                self.logger.info("Check the logfile for more information on why it failed")


            ncdump_max_cmd_parts = [self.ncdump_exe, ' ', base_nc_dir,
                                    '/max.nc > ', max_txt_path]
            ncdump_max_cmd = ''.join(ncdump_max_cmd_parts)
            (ret, ncdump_max_cmd) = self.cmdrunner.run_cmd \
                (ncdump_max_cmd, env=self.env, ismetcmd=False, run_inshell=True)

            if ret != 0:
                self.log_error(f"Command returned a non-zero return code: {ncdump_max_cmd}")
                self.logger.info("Check the logfile for more information on why it failed")

            # Search for 'min' in the min.txt file.
            try:
                with open(min_txt_path, 'r') as fmin:
                    for line in fmin:
                        min_match = re.match(r'\s*min\s*=\s([-+]?\d*\.*\d*)',
                                             line)
                        if min_match:
                            cur_min = float(min_match.group(1))
                            if cur_min < vmin:
                                vmin = cur_min
            except IOError:
                msg = ("cannot open the min text file")
                self.log_error(msg)

            # Search for 'max' in the max.txt file.
            try:
                with open(max_txt_path, 'r') as fmax:
                    for line in fmax:
                        max_match = re.match(r'\s*max\s*=\s([-+]?\d*\.*\d*)',
                                             line)
                        if max_match:
                            cur_max = float(max_match.group(1))
                            if cur_max > vmax:
                                vmax = cur_max
            except IOError:
                msg = ("cannot open the max text file")
                self.log_error(msg)

            # Clean up min.nc, min.txt, max.nc and max.txt temporary files.
            util.cleanup_temporary_files(min_temporary_files)
            util.cleanup_temporary_files(max_temporary_files)

        return vmin, vmax

    @staticmethod
    def get_var_ncfiles(do_fhr_by_range, cur_var, nc_list):
        """! Retrieve only the netCDF files corresponding to this statistic
            and variable pairing.

            Args:
                @param do_fhr_by_range: The boolean value indicating whether
                                     series analysis was performed on a range
                                     of forecast hours (True), or on a group of
                                     forecast hours (False)

                @param cur_var:  The variable of interest.
                @param nc_list:  The list of all netCDF files that were
                                 generated by the MET utility
                                 series_analysis.

            Returns:
                var_ncfiles: A list of netCDF files that
                                  correspond to this variable.
        """

        # Create the regex to retrieve the variable name.
        # The variable is contained in the netCDF file name.
        var_ncfiles = []
        if do_fhr_by_range:
            var_regex_parts = [".*series_F[0-9]{3}_", cur_var,
                               "_[0-9a-zA-Z]+.*nc"]
        else:
            var_regex_parts = [".*series_F[0-9]{3}_to_F[0-9]{3}_", cur_var,
                               "_[0-9a-zA-Z]+.*nc"]

        var_regex = ''.join(var_regex_parts)
        for cur_nc in nc_list:
            # Determine the variable from the filename
            match = re.match(var_regex, cur_nc)
            if match:
                var_ncfiles.append(cur_nc)

        return var_ncfiles

    def retrieve_nc_files(self, do_fhr_by_range):
        """! Retrieve all the netCDF files created by MET series_analysis.

        Args:
            @param do_fhr_by_range:  Boolean value, True if series analysis was
                                     performed on range.  False otherwise.
        Returns:
            nc_list:      A list of the netCDF files (full path) created
                          when the MET series analysis binary was invoked.
        """

        nc_list = []

        if do_fhr_by_range:
            filename_regex = "series_F[0-9]{3}.*nc"
        else:
            filename_regex = "series_F[0-9]{3}_to_F[0-9]{3}_*.*nc"
            # filename_regex = "series_F[0-9]{3}.*nc"

        # Get a list of all the series_F* directories
        # Use the met_utils function get_dirs to get only
        # the directories, as we are also generating
        # ASCII tmp_fcst and tmp_anly files in the
        # same directory, which can cause problems if included in
        # the series_dir_list.
        series_dir_list = util.get_dirs(self.series_lead_out_dir)

        # Iterate through each of these series subdirectories
        # and create a list of all the netCDF files (full file path).
        for series_dir in series_dir_list:
            full_path = os.path.join(self.series_lead_out_dir, series_dir)

            # Get a list of all the netCDF files for this subdirectory.
            nc_files_list = [f for f in os.listdir(full_path) if
                             os.path.isfile(os.path.join(full_path, f))]
            for cur_nc in nc_files_list:
                match = re.match(filename_regex, cur_nc)
                if match:
                    nc_file = os.path.join(full_path, cur_nc)
                    nc_list.append(nc_file)

        if not nc_list:
            self.logger.warn("empty nc_list returned")

        return nc_list

    def retrieve_fhr_tiles(self, tile_list, type_regex):
        """! Retrieves only the gridded tile files that
            correspond to the type.

            Args:
              @param tile_list:  List of tiles (full filepath).
              @param type_regex: The regex that corresponds to the tile
                        filename for this type

            Returns:
            fhr_tiles (string):  A string of gridded tile names
                                 separated by newlines
        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        fhr_tiles = ''
        for cur_tile in tile_list:
            match = re.match(type_regex, cur_tile)
            if not match:
                msg = ("No matching storm id found, exiting...")
                self.log_error(msg)
                return ''

            fhr_tiles += cur_tile
            fhr_tiles += '\n'

        return fhr_tiles

    def find_matching_tile(self, fcst_file, anly_tiles):
        """! Find the corresponding ANLY 30x30 tile file to the
            fcst tile file.
            Args:
              @param fcst_file :  The fcst file (full path) that
                                  is used to derive the corresponding
                                  analysis file name.
              @param anly_tiles : The list of all available 30x30 analysis
                                  tiles.

            Returns:
              anly_from_fcst (string): The name of the analysis tile file
                                       that corresponds to the same lead
                                       time as the input fcst tile.
        """

        # Derive the ANLY file name from the FCST file.
        anly_from_fcst = re.sub(r'FCST', 'ANLY', fcst_file)

        if anly_from_fcst in anly_tiles:
            return anly_from_fcst
        else:
            self.logger.debug("No corresponding analysis file found: " +
                              anly_from_fcst)
            return None

    @staticmethod
    def get_anly_or_fcst_files(filedir, file_type, filename_regex,
                               cur_fhr):
        """! Get all the ANLY or FCST files by walking
            through the directories starting at filedir.

            Args:
              @param filedir:  The topmost directory from which the
                               search begins.
              @param file_type:  FCST or ANLY
              @param filename_regex:  The regular expression that
                                      defines the naming format
                                      of the files of interest.

              @param cur_fhr:  The current forecast hour for which we need to
                               find the corresponding file

            Returns:
                file_paths (string): a list of filenames (with full filepath)
        """

        file_paths = []

        # Convert cur_fhr to a string that has zero padding/filling
        cur_fhr_str = (str(cur_fhr)).zfill(3)

        # Walk the tree
        for root, _, files in os.walk(filedir):
            for filename in files:
                # add it to the list only if it is a match
                # to the specified format
                # prog = re.compile(filename_regex)
                match = re.match(filename_regex, filename)
                if match:
                    # Now match based on the current forecast hour
                    if file_type == 'FCST':
                        match_fhr = re.match(r'.*FCST_TILE_F([0-9]{3}).*',
                                             match.group())

                    elif file_type == 'ANLY':
                        match_fhr = re.match(r'.*ANLY_TILE_F([0-9]{3}).*',
                                             match.group())

                    if match_fhr:
                        if match_fhr.group(1) == cur_fhr_str:
                            # Join the two strings to form the full
                            # file path.
                            filepath = os.path.join(root, filename)
                            file_paths.append(filepath)
                else:
                    continue
        return file_paths


    def generate_plots(self, do_fhr_by_range):
        """! Generate the plots and animation GIFs for the series analysis
             results.

             Args:
                 @param do_fhr_by_range   The boolean flag which indicates
                                       whether series analysis is to be
                                       performed for the entire range of fhrs,
                                       (True), or by groups of fhrs (False).

             Returns: None

        """

        # Generate a plot for each variable, statistic, and lead time.
        # First, retrieve all the netCDF files that were generated
        # above by the run series analysis.
        self.logger.info('GENERATING PLOTS...')

        # Retrieve a list of all the netCDF files generated by the
        # filtering.
        nc_list = self.retrieve_nc_files(do_fhr_by_range)

        # Check that we have netCDF files, if not, something went
        # wrong.
        if not nc_list:
            self.log_error("could not find any netCDF files to convert"
                              " to PS and PNG.  Exiting...")
            sys.exit(1)
        else:
            msg = ("Number of nc files found to convert to PS and PNG  : " +
                   str(len(nc_list)))
            self.logger.debug(msg)

        # Get the name and level to set the NAME and LEVEL
        # environment variables that
        # are needed by the MET series analysis binary.
        full_vars_list = feature_util.retrieve_var_name_levels(self.config)
        for cur_var in full_vars_list:
            name, level = cur_var
            self.add_env_var('LEVEL', level)
            self.add_env_var('NAME', name)

            # Retrieve only those netCDF files that correspond to
            # the current variable.
            nc_var_list = self.get_var_ncfiles(do_fhr_by_range, name, nc_list)
            if not nc_var_list:
                self.logger.debug("nc_var_list is empty for " + name +
                                  "_" + level + ", check for next variable...")
                continue

            # Iterate over the statistics, setting the CUR_STAT
            # environment variable...
            for cur_stat in self.stat_list:
                # Set environment variable required by MET
                # application Plot_Data_Plane.
                self.add_env_var('CUR_STAT', cur_stat)
                vmin, vmax = self.get_netcdf_min_max(do_fhr_by_range,
                                                     nc_var_list,
                                                     cur_stat)
                msg = ("Plotting range for " + name + " " + level + " " +
                       cur_stat + ":  " + str(vmin) + " to " + str(vmax))
                self.logger.debug(msg)

                # Plot the output for each time
                # DEBUG
                self.logger.info("Create PS and PNG")

                # pylint:disable=anomalous-backslash-in-string
                # This backslash is necessary in the regex.

                for cur_nc in nc_var_list:
                    # The postscript files are derived from
                    # each netCDF file. The postscript filename is
                    # created by replacing the '.nc' extension
                    # with '_<cur_stat>.ps'. The png file is created
                    # by replacing the '.ps'
                    # extension of the postscript file with '.png'.
                    repl_string = ['_', cur_stat, '.ps']
                    repl = ''.join(repl_string)
                    ps_file = re.sub(r'(\.nc)$', repl, cur_nc)

                    # Now create the PNG filename from the
                    # Postscript filename.
                    png_file = re.sub(r'(\.ps)$', '.png', ps_file)

                    # Extract the forecast hour from the netCDF
                    # filename.
                    if do_fhr_by_range:
                        match_fhr = re.match(
                            r'.*/series_F\d{3}/series_F(\d{3}).*\.nc',
                            cur_nc)
                    else:
                        match_fhr = re.match(
                            r'.*/.*/(series_F(\d{3})_'
                            r'to_F(\d{3})).*\.nc', cur_nc)

                    if match_fhr:
                        fhr = match_fhr.group(1)
                    else:
                        msg = ("netCDF file format for file: " +
                               cur_nc +
                               " is unexpected. Try next file in list...")
                        self.logger.debug(msg)
                        continue

                    # Get the max series_cnt_TOTAL value (i.e. nseries)
                    num_string = ''
                    nseries = self.get_nseries(do_fhr_by_range, cur_nc)
                    if nseries is not None:
                        num_string = f" (N = {nseries})"

                    # Create the plot data plane command based on whether
                    # the background map was requested in the
                    # param/config file.
                    if self.background_map:
                        # Flag set to True, print background map.
                        map_data = ''
                    else:
                        map_data = "map_data={source=[];}  "

                    super().set_environment_variables()

                    plot_data_plane_parts = [self.plot_data_plane_exe, ' ',
                                             cur_nc, ' ', ps_file, ' ',
                                             "'", 'name = ', '"',
                                             'series_cnt_', cur_stat, '";',
                                             'level=', r'"(\*,\*)"; ',
                                             ' ', map_data,
                                             "'", ' -title ',
                                             f"\"{self.c_dict['MODEL']} ",
                                             str(fhr),
                                             f' Forecasts{num_string}', ', ',
                                             cur_stat, ' for ', name,
                                             ' ', level,
                                             '"', ' -plot_range ', str(vmin),
                                             ' ', str(vmax)]

                    plot_data_plane_cmd = ''.join(plot_data_plane_parts)

                    # Since this wrapper is not using the CommandBuilder
                    # to build the cmd, we need to add the met verbosity
                    # level to the MET cmd created before we run
                    # the command.
                    plot_data_plane_cmd =\
                        self.cmdrunner.insert_metverbosity_opt\
                        (plot_data_plane_cmd)
                    (ret, plot_data_plane_cmd) = self.cmdrunner.run_cmd\
                        (plot_data_plane_cmd, env=self.env, app_name='plot_data_plane')

                    if ret != 0:
                        self.log_error(f"MET Command returned a non-zero return code: {plot_data_plane_cmd}")
                        self.logger.info("Check the logfile for more information on why it failed")

                    # Create the convert command.
                    convert_parts = [self.convert_exe, ' -rotate 90 ',
                                     ' -background white -flatten ',
                                     ps_file, ' ', png_file]
                    convert_cmd = ''.join(convert_parts)
                    (ret, convert_cmd) = self.cmdrunner.run_cmd(convert_cmd, env=self.env,
                                                                ismetcmd=False)

                    if ret != 0:
                        self.log_error(f"Command returned a non-zero return code: {convert_cmd}")
                        self.logger.info("Check the logfile for more information on why it failed")


    def create_animated_gifs(self, do_fhr_by_range):
        """! Creates the animated GIF files from the .png files created in
             generate_plots().

             Args:
                  @param do_fhr_by_range:  The boolean flag indicating whether
                                        series analysis was performed on the
                                        entire range (True) or on groups of
                                        forecast hours (False).
            Returns:

        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        animate_dir = os.path.join(self.series_lead_out_dir, 'series_animate')
        msg = ('Creating Animation Plots, create directory:' +
               animate_dir)
        self.logger.debug(msg)
        util.mkdir_p(animate_dir)

        # Get the name and level to set the NAME and LEVEL
        # environment variables that
        # are needed by the MET series analysis binary.
        full_vars_list = feature_util.retrieve_var_name_levels(self.config)
        for cur_var in full_vars_list:
            name, level = cur_var
            self.add_env_var('LEVEL', level)
            self.add_env_var('NAME', name)

            super().set_environment_variables()

            self.logger.info("Creating animated gifs")
            for cur_stat in self.stat_list:
                if do_fhr_by_range:
                    series_dir = '/series_F*'
                    series_fname_root = '/series_F*'
                    gif_parts = [self.convert_exe,
                                 ' -dispose Background -delay 100 ',
                                 self.series_lead_out_dir, '/', series_dir,
                                 series_fname_root, '_', name, '_',
                                 level, '_', cur_stat, '.png', '  ',
                                 animate_dir, '/series_animate_', name, '_',
                                 level, '_', cur_stat, '.gif']
                    animate_cmd = ''.join(gif_parts)
                    self.logger.debug("animate cmd: {}".format(animate_cmd))

                    (ret, animate_cmd) = self.cmdrunner.run_cmd\
                        (animate_cmd, env=self.env, ismetcmd=False,
                         run_inshell=True, log_theoutput=True)

                    if ret != 0:
                        self.log_error(f"Command returned a non-zero return code: {animate_cmd}")
                        self.logger.info("Check the logfile for more information on why it failed")

                else:
                    # For series analysis by forecast hour groups, create a
                    # list of the series analysis output for all the forecast
                    # hour groups for each variable-level-statistic
                    # combination.
                    wildcard_list = []
                    for group_label in self.fhr_group_labels:
                        wildcard_parts = [self.series_lead_out_dir, '/',
                                          group_label, '/series_F*_to_F*_',
                                          name, '_', level, '_', cur_stat,
                                          '.png']
                        wildcard = ''.join(wildcard_parts)
                        wildcard_list.append(wildcard)
                    wildcard_string = ' '.join(wildcard_list)
                    self.logger.debug("By group wildcards: " +
                                      wildcard_string)

                    gif_parts = [self.convert_exe,
                                 ' -dispose Background -delay 100 ',
                                 wildcard_string, '  ', animate_dir,
                                 '/series_animate_', name, '_', level, '_',
                                 cur_stat, '.gif']

                    animate_cmd = ''.join(gif_parts)
                    (ret, animate_cmd) = self.cmdrunner.run_cmd \
                        (animate_cmd, env=self.env, ismetcmd=False,
                         run_inshell=True, log_theoutput=True)

                    if ret != 0:
                        self.log_error(f"Command returned a non-zero return code: {animate_cmd}")
                        self.logger.info("Check the logfile for more information on why it failed")

    def apply_series_filters(self, tile_dir, init_times, series_output_dir,
                             filter_opts, staging_dir):

        """! Apply filter options, as specified in the
            param/config file.
            Args:
               @param tile_dir:  Directory where input data files reside.
                                 e.g. data which we will be applying our filter
                                 criteria.
               @param init_times:  List of init times that define the
                                   input data.
               @param series_output_dir:  The directory where the filter results
                                          will be stored.
               @param filter_opts:  The filter options to apply
               @param staging_dir:  The temporary directory where intermediate
                                      files are saved.
            Returns:
                None
        """
        # pylint: disable=too-many-arguments
        # Seven input arguments are needed to perform filtering.

        # Create temporary directory where intermediate files are saved.
        self.logger.debug("creating tmp dir for filter files: " + staging_dir)
        util.mkdir_p(staging_dir)

        for cur_init in init_times:
            # Call the tc_stat wrapper to build up the command and invoke
            # the MET tool tc_stat.
            filter_file = "filter_" + cur_init + ".tcst"
            filter_filename = os.path.join(series_output_dir,
                                           cur_init, filter_file)
            input_dict = {'init': datetime.strptime(cur_init, '%Y%m%d_%H')}
            job_args = (f'-job filter {filter_opts}'
                        f' -dump_row {filter_filename}')
            override_dict = {'TC_STAT_JOB_ARGS': job_args,
                             'TC_STAT_INIT_INCLUDE': cur_init,
                             'TC_STAT_LOOKIN_DIR': tile_dir,
                             'TC_STAT_OUTPUT_DIR': series_output_dir,
                             'TC_STAT_MATCH_POINTS': True,
                             }
            tc_stat_wrapper = TCStatWrapper(self.config,
                                            config_overrides=override_dict)
            if not tc_stat_wrapper.isOK:
                continue

            if not tc_stat_wrapper.run_at_time(input_dict):
                continue

#            tcs = TCStatWrapper(self.config)
#            tcs.build_tc_stat(series_output_dir, cur_init, tile_dir,
#                              filter_opts)

            # Check that the filter.tcst file isn't empty. If
            # it is, then use the files from extract_tiles as
            # input (tile_dir = extract_out_dir)
            if not util.file_exists(filter_filename):
                msg = ("Non-existent filter file, filter " +
                       " Never created by MET Tool tc_stat.")
                self.logger.debug(msg)
                continue
            elif os.stat(filter_filename).st_size == 0:
                msg = ("Empty filter file, filter " +
                       " options yield nothing.")
                self.logger.debug(msg)
                continue
            else:
                # Now retrieve the files corresponding to these
                # storm ids that resulted from filtering.
                sorted_storm_ids = util.get_storm_ids(filter_filename,
                                                      self.logger)

                # Retrieve the header from filter_filename to be used in
                # creating the temporary files.
                with open(filter_filename, 'r') as filter_file:
                    header = filter_file.readline()

                for cur_storm in sorted_storm_ids:
                    msg = ("Processing storm: " +
                           cur_storm + " for file: " + filter_filename)
                    self.logger.debug(msg)
                    storm_output_dir = os.path.join(series_output_dir,
                                                    cur_init, cur_storm)
                    util.mkdir_p(storm_output_dir)

                    tmp_file = "filter_" + cur_init + "_" + cur_storm
                    tmp_filename = os.path.join(staging_dir, tmp_file)
                    storm_match_list = util.grep(cur_storm, filter_filename)
                    with open(tmp_filename, "w") as tmp_file:
                        tmp_file.write(header)
                        for storm_match in storm_match_list:
                            tmp_file.write(storm_match)

                    # Create the analysis and forecast files based
                    # on the storms (defined in the tmp_filename created above)
                    # Store the analysis and forecast files in the
                    # series_output_dir.
                    feature_util.retrieve_and_regrid(tmp_filename, cur_init,
                                                     cur_storm,
                                                     series_output_dir,
                                                     self.config)

        # Check for any empty files and directories and remove them to avoid
        # any errors or performance degradation when performing
        # series analysis.
        util.prune_empty(series_output_dir, self.logger)

        # Clean up the tmp dir and create an empty one
        # in anticipation of another run.
        filter_regex = 'filter_.*'
        util.remove_staged_files(staging_dir, filter_regex, self.logger)
